{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers # type: ignore\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, dataset_path, m=3):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.m = m\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        df = pd.read_csv(self.dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
        "                                                      '_source_network_bytes', '_source_@timestamp', 'label'])\n",
        "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
        "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
        "        grouped = df.groupby('window_id')\n",
        "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
        "\n",
        "        X, y = [], []\n",
        "        for wid in window_ids:\n",
        "            window_df = grouped.get_group(wid)\n",
        "            all_ips = np.unique(np.concatenate([window_df['_source_source_ip'].unique(), window_df['_source_destination_ip'].unique()]))\n",
        "            ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
        "            num_nodes = len(all_ips)\n",
        "            node_features = np.zeros((num_nodes, 4))\n",
        "            for ip in all_ips:\n",
        "                src_bytes = window_df[window_df['_source_source_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                dst_bytes = window_df[window_df['_source_destination_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                num_src_connections = len(window_df[window_df['_source_source_ip'] == ip])\n",
        "                num_dst_connections = len(window_df[window_df['_source_destination_ip'] == ip])\n",
        "                node_features[ip_to_idx[ip]] = [src_bytes, dst_bytes, num_src_connections, num_dst_connections]\n",
        "            node_features = self.scaler.fit_transform(node_features)\n",
        "            edge_list = [[ip_to_idx[row['_source_source_ip']], ip_to_idx[row['_source_destination_ip']]] for _, row in window_df.iterrows()]\n",
        "            edge_index = np.array(edge_list).T\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            label = int((window_df['label'] == 'malicious').any())\n",
        "            X.append(graph)\n",
        "            y.append(label)\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "        for k in range(self.m, len(window_ids)):\n",
        "            seq = X[k - self.m:k]\n",
        "            sequence_label = int(any(y[k - self.m:k]))\n",
        "            X_seq.append(seq)\n",
        "            y_seq.append(sequence_label)\n",
        "\n",
        "        n = len(X_seq)\n",
        "        train_end = int(0.8 * n)\n",
        "        val_end = int(0.9 * n)\n",
        "        return (X_seq[:train_end], y_seq[:train_end], \n",
        "                X_seq[train_end:val_end], y_seq[train_end:val_end], \n",
        "                X_seq[val_end:], y_seq[val_end:])\n",
        "\n",
        "\n",
        "class GCNLayer(layers.Layer):\n",
        "    def __init__(self, units, activation='relu'):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.dense = layers.Dense(units, activation=None, use_bias=False,\n",
        "                                 kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, node_features, adj_norm, training=False):\n",
        "        h = tf.sparse.sparse_dense_matmul(adj_norm, node_features)\n",
        "        h = self.dense(h)\n",
        "        h = self.batch_norm(h, training=training)\n",
        "        h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "class GNN(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=128, output_units=64):\n",
        "        super(GNN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(hidden_units)\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.gcn2 = GCNLayer(output_units)\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        node_features, edge_indices, num_nodes = inputs\n",
        "        adj_norm = compute_normalized_adjacency(edge_indices, num_nodes)\n",
        "        x = self.gcn1(node_features, adj_norm, training=training)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.gcn2(x, adj_norm, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        embedding = tf.reduce_mean(x, axis=0)\n",
        "        return embedding\n",
        "\n",
        "def compute_normalized_adjacency(edge_indices, num_nodes):\n",
        "    num_nodes = tf.cast(num_nodes, tf.int64)\n",
        "    adj = tf.sparse.SparseTensor(indices=tf.transpose(edge_indices),\n",
        "                                 values=tf.ones([tf.shape(edge_indices)[1]], dtype=tf.float32),\n",
        "                                 dense_shape=[num_nodes, num_nodes])\n",
        "    identity_indices = tf.stack([tf.range(num_nodes), tf.range(num_nodes)], axis=1)\n",
        "    identity_sparse = tf.sparse.SparseTensor(indices=identity_indices,\n",
        "                                            values=tf.ones([num_nodes], dtype=tf.float32),\n",
        "                                            dense_shape=[num_nodes, num_nodes])\n",
        "    adj = tf.sparse.add(adj, identity_sparse)\n",
        "    degree = tf.sparse.reduce_sum(adj, axis=1)\n",
        "    degree_inv_sqrt = tf.pow(degree + 1e-9, -0.5)\n",
        "    degree_inv_sqrt = tf.where(tf.math.is_inf(degree_inv_sqrt), 0.0, degree_inv_sqrt)\n",
        "    adj_norm = tf.sparse.SparseTensor(indices=adj.indices,\n",
        "                                      values=adj.values * tf.gather(degree_inv_sqrt, adj.indices[:, 0]) * tf.gather(degree_inv_sqrt, adj.indices[:, 1]),\n",
        "                                      dense_shape=adj.dense_shape)\n",
        "    return adj_norm\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, input_size=64, hidden_size=128):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = layers.LSTM(hidden_size, return_sequences=False,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
        "                               recurrent_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.dense = layers.Dense(1, activation='sigmoid',\n",
        "                                 kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs)\n",
        "        x = self.batch_norm(x, training=training)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "class RLTrainer:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=100, batch_size=32, \n",
        "                 hidden_units=128, output_units=64, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.X_val, self.y_val = X_val, y_val\n",
        "        self.model_path = model_path\n",
        "        self.m = m\n",
        "        self.num_episodes = num_episodes\n",
        "        self.batch_size = batch_size\n",
        "        self.gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.patience = 10\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.target_gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.target_lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_gnn.set_weights(self.gnn.get_weights())\n",
        "        self.target_lstm.set_weights(self.lstm.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(2)\n",
        "        embeddings = self.get_embeddings(state)\n",
        "        q_values = self.lstm(embeddings)\n",
        "        return 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "\n",
        "    def get_embeddings(self, sequence):\n",
        "        embeddings = []\n",
        "        for graph in sequence:\n",
        "            node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "            edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "            num_nodes = node_features.shape[0]\n",
        "            embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "            embeddings.append(embedding)\n",
        "        return tf.stack(embeddings)[None, :]\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = [m[0] for m in minibatch]\n",
        "        actions = [m[1] for m in minibatch]\n",
        "        rewards = [m[2] for m in minibatch]\n",
        "        next_states = [m[3] for m in minibatch]\n",
        "        dones = [m[4] for m in minibatch]\n",
        "\n",
        "        state_embeddings = tf.stack([self.get_embeddings(s)[0] for s in states])\n",
        "        next_state_embeddings = tf.stack([self.get_embeddings(ns)[0] for ns in next_states])\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.lstm(state_embeddings)\n",
        "            target_q_values = self.target_lstm(next_state_embeddings)\n",
        "            targets = []\n",
        "            for i in range(self.batch_size):\n",
        "                target = rewards[i] if dones[i] else rewards[i] + self.gamma * tf.reduce_max(target_q_values[i])\n",
        "                targets.append(target)\n",
        "            targets = tf.convert_to_tensor(targets, dtype=tf.float32)[:, None]\n",
        "            loss = tf.keras.losses.mean_squared_error(targets, q_values)\n",
        "        \n",
        "        grads = tape.gradient(loss, self.gnn.trainable_variables + self.lstm.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.gnn.trainable_variables + self.lstm.trainable_variables))\n",
        "        \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self):\n",
        "        for episode in range(self.num_episodes):\n",
        "            train_data = list(zip(self.X_train, self.y_train))\n",
        "            random.shuffle(train_data)\n",
        "            total_reward = 0\n",
        "            for i, (sequence, label) in enumerate(train_data):\n",
        "                action = self.act(sequence)\n",
        "                reward = 1 if action == label else -1\n",
        "                total_reward += reward\n",
        "                next_idx = min(i + 1, len(train_data) - 1)\n",
        "                next_sequence, _ = train_data[next_idx]\n",
        "                done = i == len(train_data) - 1\n",
        "                self.remember(sequence, action, reward, next_sequence, done)\n",
        "                self.replay()\n",
        "\n",
        "            val_loss = 0\n",
        "            for sequence, label in zip(self.X_val, self.y_val):\n",
        "                embeddings = self.get_embeddings(sequence)\n",
        "                q_values = self.lstm(embeddings)\n",
        "                pred = 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "                val_loss += tf.keras.losses.binary_crossentropy([label], [pred]).numpy()\n",
        "            val_loss /= len(self.X_val)\n",
        "            print(f\"Episode {episode}, Reward: {total_reward}, Val Loss: {val_loss:.4f}, Epsilon: {self.epsilon:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.counter = 0\n",
        "                self.gnn.save_weights(f\"{self.model_path}_gnn.h5\")\n",
        "                self.lstm.save_weights(f\"{self.model_path}_lstm.h5\")\n",
        "                self.update_target_model()\n",
        "            else:\n",
        "                self.counter += 1\n",
        "                if self.counter >= self.patience:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        self.gnn.load_weights(f\"{self.model_path}_gnn.h5\")\n",
        "        self.lstm.load_weights(f\"{self.model_path}_lstm.h5\")\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, gnn, lstm):\n",
        "        self.gnn = gnn\n",
        "        self.lstm = lstm\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_val, y_val):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            val_loss += tf.keras.losses.binary_crossentropy([label], [y_pred]).numpy()\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        val_loss /= total\n",
        "        val_accuracy = correct / total\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "class ModelTester:\n",
        "    def __init__(self, model):\n",
        "        self.gnn, self.lstm = model\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_test, y_test):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        test_accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        return test_accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2021_malicious_data.csv'\n",
        "    test_dataset_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2022_data.csv'\n",
        "    model_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\src\\\\models\\\\trained_hybrid_model1'\n",
        "    preprocessor = DataPreprocessor(dataset_path, m=3)\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.load_and_preprocess_data()\n",
        "    trainer = RLTrainer(X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=50, batch_size=32, \n",
        "                         hidden_units=128, output_units=64)\n",
        "    trainer.train()\n",
        "    evaluator = ModelEvaluator(trainer.gnn, trainer.lstm)\n",
        "    val_loss, val_accuracy = evaluator.evaluate(X_val, y_val)\n",
        "    tester = ModelTester((trainer.gnn, trainer.lstm))\n",
        "    test_accuracy = tester.test(X_test, y_test)\n",
        "    new_test_accuracy = tester.preprocess_and_test(test_dataset_path, m=3)\n",
        "    print(f\"New Test Accuracy: {new_test_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
