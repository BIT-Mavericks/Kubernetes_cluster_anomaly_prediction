{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, dataset_path, m=3):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.m = m\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        df = pd.read_csv(self.dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
        "                                                      '_source_network_bytes', '_source_@timestamp', 'label'])\n",
        "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
        "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
        "        grouped = df.groupby('window_id')\n",
        "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
        "\n",
        "        X, y = [], []\n",
        "        for wid in window_ids:\n",
        "            window_df = grouped.get_group(wid)\n",
        "            all_ips = np.unique(np.concatenate([window_df['_source_source_ip'].unique(), window_df['_source_destination_ip'].unique()]))\n",
        "            ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
        "            num_nodes = len(all_ips)\n",
        "            node_features = np.zeros((num_nodes, 4))\n",
        "            for ip in all_ips:\n",
        "                src_bytes = window_df[window_df['_source_source_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                dst_bytes = window_df[window_df['_source_destination_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                num_src_connections = len(window_df[window_df['_source_source_ip'] == ip])\n",
        "                num_dst_connections = len(window_df[window_df['_source_destination_ip'] == ip])\n",
        "                node_features[ip_to_idx[ip]] = [src_bytes, dst_bytes, num_src_connections, num_dst_connections]\n",
        "            node_features = self.scaler.fit_transform(node_features)\n",
        "            edge_list = [[ip_to_idx[row['_source_source_ip']], ip_to_idx[row['_source_destination_ip']]] for _, row in window_df.iterrows()]\n",
        "            edge_index = np.array(edge_list).T\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            label = int((window_df['label'] == 'malicious').any())\n",
        "            X.append(graph)\n",
        "            y.append(label)\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "        for k in range(self.m, len(window_ids)):\n",
        "            seq = X[k - self.m:k]\n",
        "            sequence_label = int(any(y[k - self.m:k]))\n",
        "            X_seq.append(seq)\n",
        "            y_seq.append(sequence_label)\n",
        "\n",
        "        n = len(X_seq)\n",
        "        train_end = int(0.8 * n)\n",
        "        val_end = int(0.9 * n)\n",
        "        return (X_seq[:train_end], y_seq[:train_end], \n",
        "                X_seq[train_end:val_end], y_seq[train_end:val_end], \n",
        "                X_seq[val_end:], y_seq[val_end:])\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class GCNLayer(layers.Layer):\n",
        "    def __init__(self, units, activation='relu'):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.dense = layers.Dense(units, activation=None, use_bias=False,\n",
        "                                 kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, node_features, adj_norm, training=False):\n",
        "        h = tf.sparse.sparse_dense_matmul(adj_norm, node_features)\n",
        "        h = self.dense(h)\n",
        "        h = self.batch_norm(h, training=training)\n",
        "        h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "class GNN(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=128, output_units=64):\n",
        "        super(GNN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(hidden_units)\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.gcn2 = GCNLayer(output_units)\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        node_features, edge_indices, num_nodes = inputs\n",
        "        adj_norm = compute_normalized_adjacency(edge_indices, num_nodes)\n",
        "        x = self.gcn1(node_features, adj_norm, training=training)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.gcn2(x, adj_norm, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        embedding = tf.reduce_mean(x, axis=0)\n",
        "        return embedding\n",
        "\n",
        "def compute_normalized_adjacency(edge_indices, num_nodes):\n",
        "    num_nodes = tf.cast(num_nodes, tf.int64)\n",
        "    adj = tf.sparse.SparseTensor(indices=tf.transpose(edge_indices),\n",
        "                                 values=tf.ones([tf.shape(edge_indices)[1]], dtype=tf.float32),\n",
        "                                 dense_shape=[num_nodes, num_nodes])\n",
        "    identity_indices = tf.stack([tf.range(num_nodes), tf.range(num_nodes)], axis=1)\n",
        "    identity_sparse = tf.sparse.SparseTensor(indices=identity_indices,\n",
        "                                            values=tf.ones([num_nodes], dtype=tf.float32),\n",
        "                                            dense_shape=[num_nodes, num_nodes])\n",
        "    adj = tf.sparse.add(adj, identity_sparse)\n",
        "    degree = tf.sparse.reduce_sum(adj, axis=1)\n",
        "    degree_inv_sqrt = tf.pow(degree + 1e-9, -0.5)\n",
        "    degree_inv_sqrt = tf.where(tf.math.is_inf(degree_inv_sqrt), 0.0, degree_inv_sqrt)\n",
        "    adj_norm = tf.sparse.SparseTensor(indices=adj.indices,\n",
        "                                      values=adj.values * tf.gather(degree_inv_sqrt, adj.indices[:, 0]) * tf.gather(degree_inv_sqrt, adj.indices[:, 1]),\n",
        "                                      dense_shape=adj.dense_shape)\n",
        "    return adj_norm\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, input_size=64, hidden_size=128):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = layers.LSTM(hidden_size, return_sequences=False,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
        "                               recurrent_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.dense = layers.Dense(1, activation='sigmoid',\n",
        "                                 kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs)\n",
        "        x = self.batch_norm(x, training=training)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "class RLTrainer:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=100, batch_size=32, \n",
        "                 hidden_units=128, output_units=64, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.X_val, self.y_val = X_val, y_val\n",
        "        self.model_path = model_path\n",
        "        self.m = m\n",
        "        self.num_episodes = num_episodes\n",
        "        self.batch_size = batch_size\n",
        "        self.gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.patience = 10\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.target_gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.target_lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_gnn.set_weights(self.gnn.get_weights())\n",
        "        self.target_lstm.set_weights(self.lstm.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(2)\n",
        "        embeddings = self.get_embeddings(state)\n",
        "        q_values = self.lstm(embeddings)\n",
        "        return 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "\n",
        "    def get_embeddings(self, sequence):\n",
        "        embeddings = []\n",
        "        for graph in sequence:\n",
        "            node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "            edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "            num_nodes = node_features.shape[0]\n",
        "            embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "            embeddings.append(embedding)\n",
        "        return tf.stack(embeddings)[None, :]\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = [m[0] for m in minibatch]\n",
        "        actions = [m[1] for m in minibatch]\n",
        "        rewards = [m[2] for m in minibatch]\n",
        "        next_states = [m[3] for m in minibatch]\n",
        "        dones = [m[4] for m in minibatch]\n",
        "\n",
        "        state_embeddings = tf.stack([self.get_embeddings(s)[0] for s in states])\n",
        "        next_state_embeddings = tf.stack([self.get_embeddings(ns)[0] for ns in next_states])\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.lstm(state_embeddings)\n",
        "            target_q_values = self.target_lstm(next_state_embeddings)\n",
        "            targets = []\n",
        "            for i in range(self.batch_size):\n",
        "                target = rewards[i] if dones[i] else rewards[i] + self.gamma * tf.reduce_max(target_q_values[i])\n",
        "                targets.append(target)\n",
        "            targets = tf.convert_to_tensor(targets, dtype=tf.float32)[:, None]\n",
        "            loss = tf.keras.losses.mean_squared_error(targets, q_values)\n",
        "        \n",
        "        grads = tape.gradient(loss, self.gnn.trainable_variables + self.lstm.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.gnn.trainable_variables + self.lstm.trainable_variables))\n",
        "        \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self):\n",
        "        for episode in range(self.num_episodes):\n",
        "            train_data = list(zip(self.X_train, self.y_train))\n",
        "            random.shuffle(train_data)\n",
        "            total_reward = 0\n",
        "            for i, (sequence, label) in enumerate(train_data):\n",
        "                action = self.act(sequence)\n",
        "                reward = 1 if action == label else -1\n",
        "                total_reward += reward\n",
        "                next_idx = min(i + 1, len(train_data) - 1)\n",
        "                next_sequence, _ = train_data[next_idx]\n",
        "                done = i == len(train_data) - 1\n",
        "                self.remember(sequence, action, reward, next_sequence, done)\n",
        "                self.replay()\n",
        "\n",
        "            val_loss = 0\n",
        "            for sequence, label in zip(self.X_val, self.y_val):\n",
        "                embeddings = self.get_embeddings(sequence)\n",
        "                q_values = self.lstm(embeddings)\n",
        "                pred = 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "                val_loss += tf.keras.losses.binary_crossentropy([label], [pred]).numpy()\n",
        "            val_loss /= len(self.X_val)\n",
        "            print(f\"Episode {episode}, Reward: {total_reward}, Val Loss: {val_loss:.4f}, Epsilon: {self.epsilon:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.counter = 0\n",
        "                self.gnn.save_weights(f\"{self.model_path}_gnn.h5\")\n",
        "                self.lstm.save_weights(f\"{self.model_path}_lstm.h5\")\n",
        "                self.update_target_model()\n",
        "            else:\n",
        "                self.counter += 1\n",
        "                if self.counter >= self.patience:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        self.gnn.load_weights(f\"{self.model_path}_gnn.h5\")\n",
        "        self.lstm.load_weights(f\"{self.model_path}_lstm.h5\")\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, gnn, lstm):\n",
        "        self.gnn = gnn\n",
        "        self.lstm = lstm\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_val, y_val):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            val_loss += tf.keras.losses.binary_crossentropy([label], [y_pred]).numpy()\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        val_loss /= total\n",
        "        val_accuracy = correct / total\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "class ModelTester:\n",
        "    def __init__(self, model):\n",
        "        self.gnn, self.lstm = model\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_test, y_test):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        test_accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        return test_accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_path = 'C:\\Users\\ASUS\\Guidewire_Hackathon\\datasets\\elastic_may2021_malicious_data.csv'\n",
        "    test_dataset_path = 'C:\\Users\\ASUS\\Guidewire_Hackathon\\datasets\\elastic_may2022_data.csv'\n",
        "    model_path = 'C:\\Users\\ASUS\\Guidewire_Hackathon\\src\\models\\trained_hybrid_model1'\n",
        "    preprocessor = DataPreprocessor(dataset_path, m=3)\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.load_and_preprocess_data()\n",
        "    trainer = RLTrainer(X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=50, batch_size=32, \n",
        "                         hidden_units=128, output_units=64)\n",
        "    trainer.train()\n",
        "    evaluator = ModelEvaluator(trainer.gnn, trainer.lstm)\n",
        "    val_loss, val_accuracy = evaluator.evaluate(X_val, y_val)\n",
        "    tester = ModelTester((trainer.gnn, trainer.lstm))\n",
        "    test_accuracy = tester.test(X_test, y_test)\n",
        "    new_test_accuracy = tester.preprocess_and_test(test_dataset_path, m=3)\n",
        "    print(f\"New Test Accuracy: {new_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers  # type: ignore\n",
        "\n",
        "# ---------------- Data Preprocessing Class ----------------\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, dataset_path, m=3):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.m = m\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        # Load selected columns from CSV file\n",
        "        df = pd.read_csv(self.dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
        "                                                      '_source_network_bytes', '_source_@timestamp', 'label'])\n",
        "        # Convert timestamp column to datetime\n",
        "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
        "        # Create a window ID based on minute-level granularity\n",
        "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
        "        grouped = df.groupby('window_id')\n",
        "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
        "\n",
        "        X, y = [], []\n",
        "        # Process each time window to construct graph data\n",
        "        for wid in window_ids:\n",
        "            window_df = grouped.get_group(wid)\n",
        "            all_ips = np.unique(np.concatenate([window_df['_source_source_ip'].unique(), window_df['_source_destination_ip'].unique()]))\n",
        "            ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
        "            num_nodes = len(all_ips)\n",
        "            node_features = np.zeros((num_nodes, 4))\n",
        "            for ip in all_ips:\n",
        "                src_bytes = window_df[window_df['_source_source_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                dst_bytes = window_df[window_df['_source_destination_ip'] == ip]['_source_network_bytes'].sum()\n",
        "                num_src_connections = len(window_df[window_df['_source_source_ip'] == ip])\n",
        "                num_dst_connections = len(window_df[window_df['_source_destination_ip'] == ip])\n",
        "                node_features[ip_to_idx[ip]] = [src_bytes, dst_bytes, num_src_connections, num_dst_connections]\n",
        "            node_features = self.scaler.fit_transform(node_features)\n",
        "            edge_list = [[ip_to_idx[row['_source_source_ip']], ip_to_idx[row['_source_destination_ip']]] for _, row in window_df.iterrows()]\n",
        "            edge_index = np.array(edge_list).T\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            label = int((window_df['label'] == 'malicious').any())\n",
        "            X.append(graph)\n",
        "            y.append(label)\n",
        "\n",
        "        # Create sequences of graphs using sliding window of size m\n",
        "        X_seq, y_seq = [], []\n",
        "        for k in range(self.m, len(window_ids)):\n",
        "            seq = X[k - self.m:k]\n",
        "            sequence_label = int(any(y[k - self.m:k]))\n",
        "            X_seq.append(seq)\n",
        "            y_seq.append(sequence_label)\n",
        "\n",
        "        n = len(X_seq)\n",
        "        train_end = int(0.8 * n)\n",
        "        val_end = int(0.9 * n)\n",
        "        return (X_seq[:train_end], y_seq[:train_end], \n",
        "                X_seq[train_end:val_end], y_seq[train_end:val_end], \n",
        "                X_seq[val_end:], y_seq[val_end:])\n",
        "\n",
        "# ---------------- GCN Layer and GNN Model ----------------\n",
        "class GCNLayer(layers.Layer):\n",
        "    def __init__(self, units, activation='relu'):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.dense = layers.Dense(units, activation=None, use_bias=False,\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, node_features, adj_norm, training=False):\n",
        "        # Multiply adjacency matrix with node features, then apply dense layer, batch norm, and activation\n",
        "        h = tf.sparse.sparse_dense_matmul(adj_norm, node_features)\n",
        "        h = self.dense(h)\n",
        "        h = self.batch_norm(h, training=training)\n",
        "        h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "class GNN(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=128, output_units=64):\n",
        "        super(GNN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(hidden_units)\n",
        "        self.dropout1 = layers.Dropout(0.3)\n",
        "        self.gcn2 = GCNLayer(output_units)\n",
        "        self.dropout2 = layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        node_features, edge_indices, num_nodes = inputs\n",
        "        adj_norm = compute_normalized_adjacency(edge_indices, num_nodes)\n",
        "        x = self.gcn1(node_features, adj_norm, training=training)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.gcn2(x, adj_norm, training=training)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        # Aggregate node embeddings to get graph-level representation\n",
        "        embedding = tf.reduce_mean(x, axis=0)\n",
        "        return embedding\n",
        "\n",
        "def compute_normalized_adjacency(edge_indices, num_nodes):\n",
        "    # Compute normalized adjacency matrix for GCN\n",
        "    num_nodes = tf.cast(num_nodes, tf.int64)\n",
        "    adj = tf.sparse.SparseTensor(indices=tf.transpose(edge_indices),\n",
        "                                 values=tf.ones([tf.shape(edge_indices)[1]], dtype=tf.float32),\n",
        "                                 dense_shape=[num_nodes, num_nodes])\n",
        "    identity_indices = tf.stack([tf.range(num_nodes), tf.range(num_nodes)], axis=1)\n",
        "    identity_sparse = tf.sparse.SparseTensor(indices=identity_indices,\n",
        "                                             values=tf.ones([num_nodes], dtype=tf.float32),\n",
        "                                             dense_shape=[num_nodes, num_nodes])\n",
        "    adj = tf.sparse.add(adj, identity_sparse)\n",
        "    degree = tf.sparse.reduce_sum(adj, axis=1)\n",
        "    degree_inv_sqrt = tf.pow(degree + 1e-9, -0.5)\n",
        "    degree_inv_sqrt = tf.where(tf.math.is_inf(degree_inv_sqrt), 0.0, degree_inv_sqrt)\n",
        "    adj_norm = tf.sparse.SparseTensor(indices=adj.indices,\n",
        "                                      values=adj.values * tf.gather(degree_inv_sqrt, adj.indices[:, 0]) * tf.gather(degree_inv_sqrt, adj.indices[:, 1]),\n",
        "                                      dense_shape=adj.dense_shape)\n",
        "    return adj_norm\n",
        "\n",
        "# ---------------- LSTM Model ----------------\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, input_size=64, hidden_size=128):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = layers.LSTM(hidden_size, return_sequences=False,\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(1e-3),\n",
        "                                recurrent_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.dense = layers.Dense(1, activation='sigmoid',\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
        "        self.batch_norm = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs)\n",
        "        x = self.batch_norm(x, training=training)\n",
        "        x = self.dense(x)\n",
        "        return x\n",
        "\n",
        "# ---------------- RL Trainer (Hybrid Model Trainer using RL) ----------------\n",
        "class RLTrainer:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=50, batch_size=32, \n",
        "                 hidden_units=128, output_units=64, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.X_val, self.y_val = X_val, y_val\n",
        "        self.model_path = model_path\n",
        "        self.m = m\n",
        "        self.num_episodes = num_episodes\n",
        "        self.batch_size = batch_size\n",
        "        self.gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.patience = 10\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.target_gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.target_lstm = LSTMModel(input_size=output_units, hidden_size=128)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # Update target model weights\n",
        "        self.target_gnn.set_weights(self.gnn.get_weights())\n",
        "        self.target_lstm.set_weights(self.lstm.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # Store experience in replay memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(2)\n",
        "        embeddings = self.get_embeddings(state)\n",
        "        q_values = self.lstm(embeddings)\n",
        "        return 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "\n",
        "    def get_embeddings(self, sequence):\n",
        "        # Compute graph embeddings for each graph in the sequence using the GNN\n",
        "        embeddings = []\n",
        "        for graph in sequence:\n",
        "            node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "            edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "            num_nodes = node_features.shape[0]\n",
        "            embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "            embeddings.append(embedding)\n",
        "        return tf.stack(embeddings)[None, :]\n",
        "\n",
        "    def replay(self):\n",
        "        # Perform experience replay for training\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = [m[0] for m in minibatch]\n",
        "        actions = [m[1] for m in minibatch]\n",
        "        rewards = [m[2] for m in minibatch]\n",
        "        next_states = [m[3] for m in minibatch]\n",
        "        dones = [m[4] for m in minibatch]\n",
        "\n",
        "        state_embeddings = tf.stack([self.get_embeddings(s)[0] for s in states])\n",
        "        next_state_embeddings = tf.stack([self.get_embeddings(ns)[0] for ns in next_states])\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.lstm(state_embeddings)\n",
        "            target_q_values = self.target_lstm(next_state_embeddings)\n",
        "            targets = []\n",
        "            for i in range(self.batch_size):\n",
        "                target = rewards[i] if dones[i] else rewards[i] + self.gamma * tf.reduce_max(target_q_values[i])\n",
        "                targets.append(target)\n",
        "            targets = tf.convert_to_tensor(targets, dtype=tf.float32)[:, None]\n",
        "            loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(targets, q_values))\n",
        "        grads = tape.gradient(loss, self.gnn.trainable_variables + self.lstm.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.gnn.trainable_variables + self.lstm.trainable_variables))\n",
        "        \n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def train(self):\n",
        "        # Training loop over episodes\n",
        "        for episode in range(self.num_episodes):\n",
        "            train_data = list(zip(self.X_train, self.y_train))\n",
        "            random.shuffle(train_data)\n",
        "            total_reward = 0\n",
        "            for i, (sequence, label) in enumerate(train_data):\n",
        "                action = self.act(sequence)\n",
        "                reward = 1 if action == label else -1\n",
        "                total_reward += reward\n",
        "                next_idx = min(i + 1, len(train_data) - 1)\n",
        "                next_sequence, _ = train_data[next_idx]\n",
        "                done = i == len(train_data) - 1\n",
        "                self.remember(sequence, action, reward, next_sequence, done)\n",
        "                self.replay()\n",
        "\n",
        "            val_loss = 0\n",
        "            for sequence, label in zip(self.X_val, self.y_val):\n",
        "                embeddings = self.get_embeddings(sequence)\n",
        "                q_values = self.lstm(embeddings)\n",
        "                pred = 0 if q_values.numpy()[0][0] < 0.5 else 1\n",
        "                val_loss += tf.keras.losses.binary_crossentropy([label], [pred]).numpy()\n",
        "            val_loss /= len(self.X_val)\n",
        "            print(f\"Episode {episode}, Reward: {total_reward}, Val Loss: {val_loss:.4f}, Epsilon: {self.epsilon:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.counter = 0\n",
        "                self.gnn.save_weights(f\"{self.model_path}_gnn.h5\")\n",
        "                self.lstm.save_weights(f\"{self.model_path}_lstm.h5\")\n",
        "                self.update_target_model()\n",
        "            else:\n",
        "                self.counter += 1\n",
        "                if self.counter >= self.patience:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "        self.gnn.load_weights(f\"{self.model_path}_gnn.h5\")\n",
        "        self.lstm.load_weights(f\"{self.model_path}_lstm.h5\")\n",
        "\n",
        "# ---------------- Model Evaluator ----------------\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, gnn, lstm):\n",
        "        self.gnn = gnn\n",
        "        self.lstm = lstm\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        # Evaluate model on validation set\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_val, y_val):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            val_loss += tf.keras.losses.binary_crossentropy([label], [y_pred]).numpy()\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        val_loss /= total\n",
        "        val_accuracy = correct / total\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "# ---------------- Model Tester ----------------\n",
        "class ModelTester:\n",
        "    def __init__(self, model):\n",
        "        self.gnn, self.lstm = model\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        # Evaluate model on test set and print accuracy\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_test, y_test):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        test_accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        return test_accuracy\n",
        "\n",
        "    def preprocess_and_test(self, test_dataset_path, m=3):\n",
        "        # Load test dataset, preprocess and create dummy graph sequences (placeholder)\n",
        "        df = pd.read_csv(test_dataset_path)\n",
        "        X_test = []\n",
        "        y_test = []\n",
        "        for i in range(0, len(df) - m, m):\n",
        "            sequence = []\n",
        "            for j in range(m):\n",
        "                graph = {\n",
        "                    'x': df.iloc[i+j][['feature1', 'feature2', 'feature3']].values.astype(np.float32),\n",
        "                    'edge_index': np.array([[0, 1], [1, 0]], dtype=np.int64)\n",
        "                }\n",
        "                sequence.append(graph)\n",
        "            X_test.append(sequence)\n",
        "            y_test.append(int(df.iloc[i]['label'] == 'malicious'))\n",
        "        return self.test(X_test, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2021_malicious_data.csv'\n",
        "    test_dataset_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2022_data.csv'\n",
        "    model_path = 'C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\src\\\\models\\\\trained_hybrid_model1'\n",
        "    preprocessor = DataPreprocessor(dataset_path, m=3)\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.load_and_preprocess_data()\n",
        "    trainer = RLTrainer(X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=50, batch_size=32, \n",
        "                         hidden_units=128, output_units=64)\n",
        "    trainer.train()\n",
        "    evaluator = ModelEvaluator(trainer.gnn, trainer.lstm)\n",
        "    val_loss, val_accuracy = evaluator.evaluate(X_val, y_val)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    tester = ModelTester((trainer.gnn, trainer.lstm))\n",
        "    test_accuracy = tester.test(X_test, y_test)\n",
        "    new_test_accuracy = tester.preprocess_and_test(test_dataset_path, m=3)\n",
        "    print(f\"New Test Accuracy: {new_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_preprocessing.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DataPreprocessor:\n",
        "    def __init__(self, dataset_path, m=3):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.m = m\n",
        "\n",
        "    def load_and_preprocess_data(self):\n",
        "        df = pd.read_csv(self.dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
        "                                                     '_source_network_bytes', '_source_@timestamp', 'label'])\n",
        "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
        "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
        "        grouped = df.groupby('window_id')\n",
        "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
        "\n",
        "        X, y = [], []\n",
        "        for wid in window_ids:\n",
        "            window_df = grouped.get_group(wid)\n",
        "            all_ips = np.unique(np.concatenate([window_df['_source_source_ip'], window_df['_source_destination_ip']]))\n",
        "            ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
        "            num_nodes = len(all_ips)\n",
        "            node_features = np.zeros((num_nodes, 4))  # [src_bytes, dst_bytes, src_conn, dst_conn]\n",
        "            for _, row in window_df.iterrows():\n",
        "                src_idx = ip_to_idx[row['_source_source_ip']]\n",
        "                dst_idx = ip_to_idx[row['_source_destination_ip']]\n",
        "                node_features[src_idx, 0] += row['_source_network_bytes']  # src_bytes\n",
        "                node_features[dst_idx, 1] += row['_source_network_bytes']  # dst_bytes\n",
        "                node_features[src_idx, 2] += 1  # src_conn\n",
        "                node_features[dst_idx, 3] += 1  # dst_conn\n",
        "            edge_index = np.array([[ip_to_idx[row['_source_source_ip']], ip_to_idx[row['_source_destination_ip']]] \n",
        "                                   for _, row in window_df.iterrows()]).T\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            label = int((window_df['label'] == 'malicious').any())\n",
        "            X.append(graph)\n",
        "            y.append(label)\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "        for k in range(self.m, len(window_ids)):\n",
        "            seq = X[k - self.m:k]\n",
        "            sequence_label = int(any(y[k - self.m:k]))\n",
        "            X_seq.append(seq)\n",
        "            y_seq.append(sequence_label)\n",
        "\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(X_seq, y_seq, test_size=0.1, stratify=y_seq, random_state=42)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, stratify=y_temp, random_state=42)\n",
        "\n",
        "        return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GNN.py\n",
        "import tensorflow as tf\n",
        "\n",
        "class GCNLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, activation='relu'):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(units, activation=None, use_bias=False,\n",
        "                                          kernel_regularizer=tf.keras.regularizers.l2(1e-2))\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def call(self, node_features, adj_norm):\n",
        "        h = tf.sparse.sparse_dense_matmul(adj_norm, node_features)\n",
        "        h = self.dense(h)\n",
        "        h = self.activation(h)\n",
        "        return h\n",
        "\n",
        "class GNN(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=64, output_units=16):\n",
        "        super(GNN, self).__init__()\n",
        "        self.gcn1 = GCNLayer(hidden_units)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.gcn2 = GCNLayer(output_units)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        node_features, edge_indices, num_nodes = inputs\n",
        "        adj_norm = compute_normalized_adjacency(edge_indices, num_nodes)\n",
        "        x = self.gcn1(node_features, adj_norm)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.gcn2(x, adj_norm)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        embedding = tf.reduce_mean(x, axis=0)\n",
        "        return embedding\n",
        "\n",
        "def compute_normalized_adjacency(edge_indices, num_nodes):\n",
        "    num_nodes = tf.cast(num_nodes, tf.int64)\n",
        "    adj = tf.sparse.SparseTensor(indices=tf.transpose(edge_indices),\n",
        "                                 values=tf.ones([tf.shape(edge_indices)[1]], dtype=tf.float32),\n",
        "                                 dense_shape=[num_nodes, num_nodes])\n",
        "    identity_indices = tf.stack([tf.range(num_nodes), tf.range(num_nodes)], axis=1)\n",
        "    identity_sparse = tf.sparse.SparseTensor(indices=identity_indices,\n",
        "                                            values=tf.ones([num_nodes], dtype=tf.float32),\n",
        "                                            dense_shape=[num_nodes, num_nodes])\n",
        "    adj = tf.sparse.add(adj, identity_sparse)\n",
        "    degree = tf.sparse.reduce_sum(adj, axis=1)\n",
        "    degree_inv_sqrt = tf.pow(degree + 1e-9, -0.5)\n",
        "    degree_inv_sqrt = tf.where(tf.math.is_inf(degree_inv_sqrt), 0.0, degree_inv_sqrt)\n",
        "    adj_norm = tf.sparse.SparseTensor(indices=adj.indices,\n",
        "                                      values=adj.values * tf.gather(degree_inv_sqrt, adj.indices[:, 0]) * tf.gather(degree_inv_sqrt, adj.indices[:, 1]),\n",
        "                                      dense_shape=adj.dense_shape)\n",
        "    return adj_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM.py\n",
        "import tensorflow as tf\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, input_size=16, hidden_size=32):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=False,\n",
        "                                        kernel_regularizer=tf.keras.regularizers.l2(1e-2),\n",
        "                                        recurrent_regularizer=tf.keras.regularizers.l2(1e-2))\n",
        "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid',\n",
        "                                          kernel_regularizer=tf.keras.regularizers.l2(1e-2))\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rl_agent.py\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size=16, action_size=2):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis, :])\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = np.array([t[0] for t in minibatch])\n",
        "        actions = np.array([t[1] for t in minibatch])\n",
        "        rewards = np.array([t[2] for t in minibatch])\n",
        "        next_states = np.array([t[3] for t in minibatch])\n",
        "        dones = np.array([t[4] for t in minibatch])\n",
        "\n",
        "        targets = self.model.predict(states)\n",
        "        targets_next = self.target_model.predict(next_states)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            targets[i][actions[i]] = rewards[i] + self.gamma * np.amax(targets_next[i]) * (1 - dones[i])\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# training.py\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=100, batch_size=32, hidden_units=64, output_units=16):\n",
        "        self.X_train, self.y_train = X_train, y_train\n",
        "        self.X_val, self.y_val = X_val, y_val\n",
        "        self.model_path = model_path\n",
        "        self.m = m\n",
        "        self.num_episodes = num_episodes\n",
        "        self.batch_size = batch_size\n",
        "        self.gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
        "        self.lstm = LSTMModel(input_size=output_units, hidden_size=32)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.rl_agent = DQNAgent(state_size=output_units, action_size=2)\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.patience = 10\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "\n",
        "    def train(self):\n",
        "        for episode in range(self.num_episodes):\n",
        "            train_data = list(zip(self.X_train, self.y_train))\n",
        "            random.shuffle(train_data)\n",
        "            train_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for i in range(0, len(train_data), self.batch_size):\n",
        "                batch = train_data[i:i + self.batch_size]\n",
        "                batch_X = [seq for seq, _ in batch]\n",
        "                batch_y = tf.convert_to_tensor([label for _, label in batch], dtype=tf.float32)[:, None]\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    batch_embeddings = []\n",
        "                    for sequence in batch_X:\n",
        "                        sequence_embeddings = []\n",
        "                        for graph in sequence:\n",
        "                            node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                            edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                            num_nodes = node_features.shape[0]\n",
        "                            embedding = self.gnn((node_features, edge_indices, num_nodes), training=True)\n",
        "                            sequence_embeddings.append(embedding)\n",
        "                        batch_embeddings.append(tf.stack(sequence_embeddings))\n",
        "                    batch_embeddings = tf.stack(batch_embeddings)\n",
        "                    y_pred = self.lstm(batch_embeddings, training=True)\n",
        "                    \n",
        "                    # RL integration\n",
        "                    state = tf.reduce_mean(batch_embeddings, axis=1).numpy()\n",
        "                    action = self.rl_agent.act(state[0])\n",
        "                    reward = -tf.keras.losses.binary_crossentropy(batch_y, y_pred).numpy().mean()\n",
        "                    next_state = tf.reduce_mean(batch_embeddings, axis=1).numpy()\n",
        "                    done = (episode == self.num_episodes - 1)\n",
        "                    self.rl_agent.remember(state[0], action, reward, next_state[0], done)\n",
        "                    \n",
        "                    if len(self.rl_agent.memory) > self.batch_size:\n",
        "                        self.rl_agent.replay(self.batch_size)\n",
        "\n",
        "                    class_weights = {0: 1.0, 1: 5.0}\n",
        "                    weights = tf.gather(tf.constant([class_weights[0], class_weights[1]]), tf.cast(batch_y, tf.int32))\n",
        "                    loss = tf.keras.losses.binary_crossentropy(batch_y, y_pred) * weights\n",
        "                    loss = tf.reduce_mean(loss)\n",
        "\n",
        "                grads = tape.gradient(loss, self.gnn.trainable_variables + self.lstm.trainable_variables)\n",
        "                self.optimizer.apply_gradients(zip(grads, self.gnn.trainable_variables + self.lstm.trainable_variables))\n",
        "                train_loss += loss.numpy()\n",
        "                train_correct += tf.reduce_sum(tf.cast(tf.equal(tf.cast(y_pred > 0.5, tf.float32), batch_y), tf.int32)).numpy()\n",
        "                train_total += len(batch_y)\n",
        "\n",
        "            train_loss /= (len(train_data) + self.batch_size - 1) // self.batch_size\n",
        "            train_acc = train_correct / train_total\n",
        "\n",
        "            val_data = list(zip(self.X_val, self.y_val))\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            for i in range(0, len(val_data), self.batch_size):\n",
        "                batch = val_data[i:i + self.batch_size]\n",
        "                batch_X = [seq for seq, _ in batch]\n",
        "                batch_y = tf.convert_to_tensor([label for _, label in batch], dtype=tf.float32)[:, None]\n",
        "                batch_embeddings = []\n",
        "                for sequence in batch_X:\n",
        "                    sequence_embeddings = []\n",
        "                    for graph in sequence:\n",
        "                        node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                        edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                        num_nodes = node_features.shape[0]\n",
        "                        embedding = self.gnn((node_features, edge_indices, num_nodes), training=False)\n",
        "                        sequence_embeddings.append(embedding)\n",
        "                    batch_embeddings.append(tf.stack(sequence_embeddings))\n",
        "                batch_embeddings = tf.stack(batch_embeddings)\n",
        "                y_pred = self.lstm(batch_embeddings, training=False)\n",
        "                val_loss += tf.keras.losses.binary_crossentropy(batch_y, y_pred).numpy().mean()\n",
        "                val_correct += tf.reduce_sum(tf.cast(tf.equal(tf.cast(y_pred > 0.5, tf.float32), batch_y), tf.int32)).numpy()\n",
        "                val_total += len(batch_y)\n",
        "\n",
        "            val_loss /= (len(val_data) + self.batch_size - 1) // self.batch_size\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            self.train_losses.append(train_loss)\n",
        "            self.val_losses.append(val_loss)\n",
        "            print(f\"Episode {episode}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.counter = 0\n",
        "                self.gnn.save_weights(f\"{self.model_path}_gnn.h5\")\n",
        "                self.lstm.save_weights(f\"{self.model_path}_lstm.h5\")\n",
        "                self.rl_agent.save(f\"{self.model_path}_dqn.h5\")\n",
        "            else:\n",
        "                self.counter += 1\n",
        "                if self.counter >= self.patience:\n",
        "                    print(\"Early stopping triggered\")\n",
        "                    break\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                self.rl_agent.update_target_model()\n",
        "\n",
        "        self.gnn.load_weights(f\"{self.model_path}_gnn.h5\")\n",
        "        self.lstm.load_weights(f\"{self.model_path}_lstm.h5\")\n",
        "        self.rl_agent.load(f\"{self.model_path}_dqn.h5\")\n",
        "\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Val Loss')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# evaluation.py\n",
        "import tensorflow as tf\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, gnn, lstm):\n",
        "        self.gnn = gnn\n",
        "        self.lstm = lstm\n",
        "\n",
        "    def evaluate(self, X_val, y_val):\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_val, y_val):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes), training=False)\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings, training=False).numpy()[0][0]\n",
        "            val_loss += tf.keras.losses.binary_crossentropy([label], [y_pred]).numpy()\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        val_loss /= total\n",
        "        val_accuracy = correct / total\n",
        "        return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing.py\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class ModelTester:\n",
        "    def __init__(self, model):\n",
        "        self.gnn, self.lstm = model\n",
        "\n",
        "    def test(self, X_test, y_test):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for sequence, label in zip(X_test, y_test):\n",
        "            embeddings = []\n",
        "            for graph in sequence:\n",
        "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
        "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
        "                num_nodes = node_features.shape[0]\n",
        "                embedding = self.gnn((node_features, edge_indices, num_nodes), training=False)\n",
        "                embeddings.append(embedding)\n",
        "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
        "            y_pred = self.lstm(sequence_embeddings, training=False).numpy()[0][0]\n",
        "            if (y_pred > 0.5) == label:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        test_accuracy = correct / total\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "        return test_accuracy\n",
        "\n",
        "    def preprocess_and_test(self, dataset_path, m=3):\n",
        "        df = pd.read_csv(dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
        "                                                '_source_network_bytes', '_source_@timestamp', 'label'], delimiter=';')\n",
        "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
        "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
        "        grouped = df.groupby('window_id')\n",
        "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
        "\n",
        "        X, y = [], []\n",
        "        for wid in window_ids:\n",
        "            window_df = grouped.get_group(wid)\n",
        "            all_ips = np.unique(np.concatenate([window_df['_source_source_ip'], window_df['_source_destination_ip']]))\n",
        "            ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
        "            num_nodes = len(all_ips)\n",
        "            node_features = np.zeros((num_nodes, 4))\n",
        "            for _, row in window_df.iterrows():\n",
        "                src_idx = ip_to_idx[row['_source_source_ip']]\n",
        "                dst_idx = ip_to_idx[row['_source_destination_ip']]\n",
        "                node_features[src_idx, 0] += row['_source_network_bytes']\n",
        "                node_features[dst_idx, 1] += row['_source_network_bytes']\n",
        "                node_features[src_idx, 2] += 1\n",
        "                node_features[dst_idx, 3] += 1\n",
        "            edge_index = np.array([[ip_to_idx[row['_source_source_ip']], ip_to_idx[row['_source_destination_ip']]] \n",
        "                                   for _, row in window_df.iterrows()]).T\n",
        "            graph = Data(x=node_features, edge_index=edge_index)\n",
        "            label = int((window_df['label'] == 'malicious').any())\n",
        "            X.append(graph)\n",
        "            y.append(label)\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "        for k in range(m, len(window_ids)):\n",
        "            seq = X[k - m:k]\n",
        "            sequence_label = int(any(y[k - m:k]))\n",
        "            X_seq.append(seq)\n",
        "            y_seq.append(sequence_label)\n",
        "\n",
        "        print(f\"Testing on preprocessed dataset from {dataset_path}\")\n",
        "        return self.test(X_seq, y_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training label distribution: [   0 1192]\n",
            "Validation label distribution: [  0 150]\n",
            "Test label distribution: [  0 150]\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest label distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y_test))\n\u001b[0;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(X_train, y_train, X_val, y_val, model_path, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, hidden_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, output_units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluator(trainer\u001b[38;5;241m.\u001b[39mgnn, trainer\u001b[38;5;241m.\u001b[39mlstm)\n\u001b[0;32m     24\u001b[0m val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
            "Cell \u001b[1;32mIn[8], line 61\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_agent\u001b[38;5;241m.\u001b[39mremember(state[\u001b[38;5;241m0\u001b[39m], action, reward, next_state[\u001b[38;5;241m0\u001b[39m], done)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrl_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m5.0\u001b[39m}\n\u001b[0;32m     64\u001b[0m weights \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(tf\u001b[38;5;241m.\u001b[39mconstant([class_weights[\u001b[38;5;241m0\u001b[39m], class_weights[\u001b[38;5;241m1\u001b[39m]]), tf\u001b[38;5;241m.\u001b[39mcast(batch_y, tf\u001b[38;5;241m.\u001b[39mint32))\n",
            "Cell \u001b[1;32mIn[7], line 49\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m next_states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m minibatch])\n\u001b[0;32m     47\u001b[0m dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m minibatch])\n\u001b[1;32m---> 49\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m targets_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mpredict(next_states)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\Guidewire_Hackathon\\genv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\Guidewire_Hackathon\\genv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_util.py:631\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    629\u001b[0m       grad_fn \u001b[38;5;241m=\u001b[39m func_call\u001b[38;5;241m.\u001b[39mpython_grad_func\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[0;32m    632\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradient defined for operation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    633\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (op type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    634\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn general every operation must have an associated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    635\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@tf.RegisterGradient` for correct autodiff, which this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    636\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mop is lacking. If you want to pretend this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation is a constant in your program, you may insert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.stop_gradient`. This can be useful to silence the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    639\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in cases where you know gradients are not needed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g. the forward pass of tf.custom_gradient. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    641\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see more details in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/api_docs/python/tf/custom_gradient.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop_state:\n\u001b[0;32m    644\u001b[0m   loop_state\u001b[38;5;241m.\u001b[39mEnterGradWhileContext(op, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[1;31mLookupError\u001b[0m: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient."
          ]
        }
      ],
      "source": [
        "# main.py\n",
        "import numpy as np\n",
        "# from data_preprocessing import DataPreprocessor\n",
        "# from training import ModelTrainer\n",
        "# from evaluation import ModelEvaluator\n",
        "# from testing import ModelTester\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_path = \"C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2021_malicious_data.csv\"\n",
        "    test_dataset_path = \"C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2022_data.csv\"\n",
        "    model_path = \"C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\src\\\\models\\\\trained_hybrid_model1\"\n",
        "\n",
        "    preprocessor = DataPreprocessor(dataset_path, m=3)\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.load_and_preprocess_data()\n",
        "\n",
        "    print(\"Training label distribution:\", np.bincount(y_train))\n",
        "    print(\"Validation label distribution:\", np.bincount(y_val))\n",
        "    print(\"Test label distribution:\", np.bincount(y_test))\n",
        "\n",
        "    trainer = ModelTrainer(X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=100, batch_size=32, hidden_units=64, output_units=16)\n",
        "    trainer.train()\n",
        "\n",
        "    evaluator = ModelEvaluator(trainer.gnn, trainer.lstm)\n",
        "    val_loss, val_accuracy = evaluator.evaluate(X_val, y_val)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    tester = ModelTester((trainer.gnn, trainer.lstm))\n",
        "    test_accuracy = tester.test(X_test, y_test)\n",
        "    new_test_accuracy = tester.preprocess_and_test(test_dataset_path, m=3)\n",
        "    print(f\"New Test Accuracy: {new_test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
