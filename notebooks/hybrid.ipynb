{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data  # Used for graph data structure\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, dataset_path, m=3):\n",
    "        self.dataset_path = dataset_path  # Path to CSV dataset\n",
    "        self.m = m  # Sequence length (number of graphs per sequence)\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        # Load dataset (adjust columns based on your CSV)\n",
    "        df = pd.read_csv(self.dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
    "                                                      '_source_network_bytes', '_source_@timestamp', 'label'])\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
    "        # Group by minute windows (window_id)\n",
    "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
    "        grouped = df.groupby('window_id')\n",
    "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
    "\n",
    "        # Create graph data for each window (placeholder logic)\n",
    "        X, y = [], []\n",
    "        for wid in window_ids:\n",
    "            window_df = grouped.get_group(wid)\n",
    "            # Simplified graph creation: 10 nodes with random features, 20 edges\n",
    "            # Replace with actual graph creation logic based on your data\n",
    "            graph = Data(x=np.random.rand(10, 4), edge_index=np.random.randint(0, 10, (2, 20)))\n",
    "            label = int((window_df['label'] == 'malicious').any())  # Binary label: 1 if malicious\n",
    "            X.append(graph)\n",
    "            y.append(label)\n",
    "\n",
    "        # Create sequences of m graphs\n",
    "        X_seq, y_seq = [], []\n",
    "        for k in range(self.m, len(window_ids)):\n",
    "            seq = X[k - self.m:k]\n",
    "            X_seq.append(seq)\n",
    "            y_seq.append(y[k])\n",
    "\n",
    "        # Split into train (80%), validation (10%), and test (10%) sets\n",
    "        n = len(X_seq)\n",
    "        train_end = int(0.8 * n)\n",
    "        val_end = int(0.9 * n)\n",
    "        X_train, y_train = X_seq[:train_end], y_seq[:train_end]\n",
    "        X_val, y_val = X_seq[train_end:val_end], y_seq[train_end:val_end]\n",
    "        X_test, y_test = X_seq[val_end:], y_seq[val_end:]\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n",
    "# Explain this code\n",
    "# This code defines a class DataPreprocessor that preprocesses a dataset for graph-based time series forecasting.\n",
    "# The class has a constructor that takes the path to a CSV dataset and a sequence length m as input.\n",
    "# The load_and_preprocess_data method loads the dataset, groups the data into minute windows, creates graph data for each window,\n",
    "# creates sequences of m graphs, and splits the data into train, validation, and test sets.\n",
    "# The graph data is created using random features and edges for demonstration purposes, and should be replaced with actual graph creation logic.\n",
    "# The labels are binary, with a value of 1 indicating a malicious window.\n",
    "# The method returns the train, validation, and test sets as lists of sequences of graphs and corresponding labels.\n",
    "# The code uses the pandas library for data manipulation and the torch_geometric library for graph data creation.\n",
    "# The Data class from torch_geometric is used to represent graph data in PyTorch Geometric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class GCNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation='relu'):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        # Dense layer with L2 regularization\n",
    "        print(units)\n",
    "        self.dense = tf.keras.layers.Dense(units, activation=None, use_bias=False,\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def call(self, node_features, adj_norm):\n",
    "        # GCN operation: A_norm * X * W\n",
    "        h = tf.sparse.sparse_dense_matmul(adj_norm, node_features)\n",
    "        h = self.dense(h)\n",
    "        h = self.activation(h)\n",
    "        return h\n",
    "\n",
    "class GNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_units=64, output_units=32):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcn1 = GCNLayer(hidden_units)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.6)\n",
    "        self.gcn2 = GCNLayer(output_units)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Unpack the input tuple\n",
    "        node_features, edge_indices, num_nodes = inputs\n",
    "        \n",
    "        # Compute the normalized adjacency matrix\n",
    "        adj_norm = compute_normalized_adjacency(edge_indices, num_nodes)\n",
    "        \n",
    "        # Pass node_features and adj_norm to GCN layers\n",
    "        x = self.gcn1(node_features, adj_norm)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.gcn2(x, adj_norm)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Optional: Pool node embeddings into a graph embedding\n",
    "        embedding = tf.reduce_mean(x, axis=0)  # Mean pooling\n",
    "        return embedding\n",
    "\n",
    "\n",
    "\n",
    "def compute_normalized_adjacency(edge_indices, num_nodes):\n",
    "    # Cast num_nodes to int64 for consistency with edge_indices\n",
    "    num_nodes = tf.cast(num_nodes, tf.int64)\n",
    "    \n",
    "    # Create sparse adjacency matrix from edge_indices (already int64)\n",
    "    adj = tf.sparse.SparseTensor(\n",
    "        indices=tf.transpose(edge_indices),\n",
    "        values=tf.ones([tf.shape(edge_indices)[1]], dtype=tf.float32),\n",
    "        dense_shape=[num_nodes, num_nodes]\n",
    "    )\n",
    "    \n",
    "    # Create sparse identity matrix for self-loops\n",
    "    identity_indices = tf.stack([tf.range(num_nodes), tf.range(num_nodes)], axis=1)\n",
    "    identity_values = tf.ones([num_nodes], dtype=tf.float32)\n",
    "    identity_sparse = tf.sparse.SparseTensor(\n",
    "        indices=identity_indices,\n",
    "        values=identity_values,\n",
    "        dense_shape=[num_nodes, num_nodes]\n",
    "    )\n",
    "    \n",
    "    # Add self-loops to adjacency matrix\n",
    "    adj = tf.sparse.add(adj, identity_sparse)\n",
    "    \n",
    "    # Compute degree normalization\n",
    "    degree = tf.sparse.reduce_sum(adj, axis=1)\n",
    "    degree_inv_sqrt = tf.pow(degree + 1e-9, -0.5)\n",
    "    degree_inv_sqrt = tf.where(tf.math.is_inf(degree_inv_sqrt), 0.0, degree_inv_sqrt)\n",
    "    \n",
    "    # Normalize adjacency matrix\n",
    "    adj_norm = tf.sparse.SparseTensor(\n",
    "        indices=adj.indices,\n",
    "        values=adj.values * tf.gather(degree_inv_sqrt, adj.indices[:, 0]) * tf.gather(degree_inv_sqrt, adj.indices[:, 1]),\n",
    "        dense_shape=adj.dense_shape\n",
    "    )\n",
    "    \n",
    "    return adj_norm\n",
    "\n",
    "# Explain this code in a simple way that is easy to understand and recall \n",
    "# when needed.\n",
    "# This code defines a custom Graph Convolutional Network (GCN) layer and a\n",
    "# GCN model using TensorFlow. The GCN layer is defined as a custom layer\n",
    "# that performs the graph convolution operation on the input node features\n",
    "# and adjacency matrix. The adjacency matrix is normalized using the degree\n",
    "# normalization technique. The GCN model is defined as a sequence of GCN layers\n",
    "# with ReLU activation functions. The model takes input node features, edge\n",
    "# indices, and the number of nodes in the graph. The normalized adjacency matrix\n",
    "# is computed using the edge indices and number of nodes, and passed through\n",
    "# the GCN layers to generate node embeddings. The code also includes utility\n",
    "# functions to create sparse tensors for the adjacency matrix and normalize it\n",
    "# using the degree normalization technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class LSTMModel(tf.keras.Model):\n",
    "    def __init__(self, input_size=32, hidden_size=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # LSTM with L2 regularization\n",
    "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=False,\n",
    "                                        kernel_regularizer=tf.keras.regularizers.l2(1e-2),\n",
    "                                        recurrent_regularizer=tf.keras.regularizers.l2(1e-2))\n",
    "        # Output layer with L2 regularization\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='sigmoid',\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(1e-2))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm(inputs)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# This code defines a simple LSTM (Long Short-Term Memory) model using the Keras API\n",
    "# The model consists of an LSTM layer followed by a dense layer with a sigmoid activation function\n",
    "# The LSTM layer has a hidden size of 64 and returns only the final output sequence\n",
    "# Both the LSTM and dense layers have L2 regularization with a regularization strength of 1e-4\n",
    "# The model takes input sequences of shape (batch_size, sequence_length, input_size)\n",
    "# and outputs a single prediction for each input sequence in the batch\n",
    "# The LSTM model is used for sequence data, such as time series or sequential graph data\n",
    "# The sigmoid activation function in the output layer is suitable for binary classification tasks\n",
    "# The model can be trained using binary cross-entropy loss and optimized using gradient descent algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=50, batch_size=32, hidden_units=128, output_units=32):\n",
    "        # Initialize training and validation data, model path, and training hyperparameters.\n",
    "        self.X_train, self.y_train = X_train, y_train\n",
    "        self.X_val, self.y_val = X_val, y_val\n",
    "        self.model_path = model_path\n",
    "        self.m = m\n",
    "        self.num_episodes = num_episodes\n",
    "        self.batch_size = batch_size\n",
    "        # Initialize GNN and LSTM models.\n",
    "        self.gnn = GNN(hidden_units=hidden_units, output_units=output_units)\n",
    "        self.lstm = LSTMModel(input_size=32, hidden_size=64)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "        # Early stopping parameters.\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.patience = 2\n",
    "\n",
    "    def train(self):\n",
    "        # Training loop over episodes.\n",
    "        for episode in range(self.num_episodes):\n",
    "            train_data = list(zip(self.X_train, self.y_train))\n",
    "            random.shuffle(train_data)\n",
    "            train_loss = 0\n",
    "            for i in range(0, len(train_data), self.batch_size):\n",
    "                batch = train_data[i:i + self.batch_size]\n",
    "                # Extract sequences and labels.\n",
    "                batch_X = [seq for seq, _ in batch]\n",
    "                batch_y = tf.convert_to_tensor([label for _, label in batch], dtype=tf.float32)[:, None]\n",
    "                with tf.GradientTape() as tape:\n",
    "                    batch_embeddings = []\n",
    "                    # Generate embeddings for each sequence using the GNN.\n",
    "                    for sequence in batch_X:\n",
    "                        sequence_embeddings = []\n",
    "                        for graph in sequence:\n",
    "                            node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
    "                            edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
    "                            num_nodes = node_features.shape[0]\n",
    "                            embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
    "                            sequence_embeddings.append(embedding)\n",
    "                        batch_embeddings.append(tf.stack(sequence_embeddings))\n",
    "                    batch_embeddings = tf.stack(batch_embeddings)\n",
    "                    # LSTM model processes the sequence embeddings.\n",
    "                    y_pred = self.lstm(batch_embeddings)\n",
    "                    \n",
    "                    # ----- Weighted Loss Computation -----\n",
    "                    # Set class weights; adjust these values based on the class distribution.\n",
    "                    class_weights = {0: 1.0, 1: 5.0}\n",
    "                    # Compute binary crossentropy loss.\n",
    "                    loss = tf.keras.losses.binary_crossentropy(batch_y, y_pred)\n",
    "                    # Gather weights corresponding to each label.\n",
    "                    weights = tf.gather(tf.constant([class_weights[0], class_weights[1]]), tf.cast(batch_y, tf.int32))\n",
    "                    # Apply weights and reduce to mean.\n",
    "                    loss = loss * weights\n",
    "                    loss = tf.reduce_mean(loss)\n",
    "                    # --------------------------------------\n",
    "                    \n",
    "                grads = tape.gradient(loss, self.gnn.trainable_variables + self.lstm.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.gnn.trainable_variables + self.lstm.trainable_variables))\n",
    "                train_loss += tf.reduce_mean(loss).numpy()\n",
    "            train_loss /= (len(train_data) + self.batch_size - 1) // self.batch_size  # Ceiling division for number of batches\n",
    "\n",
    "            # Validation loop.\n",
    "            val_data = list(zip(self.X_val, self.y_val))\n",
    "            val_loss = 0\n",
    "            for i in range(0, len(val_data), self.batch_size):\n",
    "                batch = val_data[i:i + self.batch_size]\n",
    "                batch_X = [seq for seq, _ in batch]\n",
    "                batch_y = tf.convert_to_tensor([label for _, label in batch], dtype=tf.float32)[:, None]\n",
    "                batch_embeddings = []\n",
    "                for sequence in batch_X:\n",
    "                    sequence_embeddings = []\n",
    "                    for graph in sequence:\n",
    "                        node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
    "                        edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
    "                        num_nodes = node_features.shape[0]\n",
    "                        embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
    "                        sequence_embeddings.append(embedding)\n",
    "                    batch_embeddings.append(tf.stack(sequence_embeddings))\n",
    "                batch_embeddings = tf.stack(batch_embeddings)\n",
    "                y_pred = self.lstm(batch_embeddings)\n",
    "                loss = tf.keras.losses.binary_crossentropy(batch_y, y_pred)\n",
    "                weights = tf.gather(tf.constant([class_weights[0], class_weights[1]]), tf.cast(batch_y, tf.int32))\n",
    "                loss = loss * weights\n",
    "                loss = tf.reduce_mean(loss)\n",
    "                val_loss += tf.reduce_mean(loss).numpy()\n",
    "            val_loss /= (len(val_data) + self.batch_size - 1) // self.batch_size\n",
    "            print(f\"Episode {episode}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.counter = 0\n",
    "                # Save best model weights.\n",
    "                self.gnn.save_weights(f\"{self.model_path}_gnn.h5\")\n",
    "                self.lstm.save_weights(f\"{self.model_path}_lstm.h5\")\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "\n",
    "        # Load best model weights after training.\n",
    "        self.gnn.load_weights(f\"{self.model_path}_gnn.h5\")\n",
    "        self.lstm.load_weights(f\"{self.model_path}_lstm.h5\")\n",
    "\n",
    "\n",
    "\n",
    "# Explain this code in a simple way that is easy to understand and recall\n",
    "# when needed.\n",
    "# This code defines a ModelTrainer class that trains a GNN-LSTM model for graph-based time series forecasting using TensorFlow.\n",
    "# The class takes training and validation data, model parameters, and training hyperparameters as input.\n",
    "# The train method trains the model for a specified number of episodes, shuffling the training data and computing the loss for each batch.\n",
    "# The model is saved at each episode, and the best model is saved based on the validation loss\n",
    "# Early stopping is implemented to prevent overfitting, with a patience of 10 episodes\n",
    "# The GNN and LSTM models are trained using binary cross-entropy loss and optimized using the Adam optimizer\n",
    "# The GNN model is used to generate node embeddings for each graph in the sequence, which are then passed to the LSTM model\n",
    "# The LSTM model processes the sequence of node embeddings and outputs a single prediction for each input sequence\n",
    "# The training process is monitored using the training and validation loss, and early stopping is triggered if the validation loss does not improve\n",
    "# The best model weights are loaded after training is completed, and the training process is terminated\n",
    "# The trained model can be used for graph-based time series forecasting tasks, such as anomaly detection or event prediction\n",
    "# The code demonstrates how to train a GNN-LSTM model for graph-based time series forecasting using TensorFlow and custom layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, gnn, lstm):\n",
    "        self.gnn = gnn\n",
    "        self.lstm = lstm\n",
    "\n",
    "    def evaluate(self, X_val, y_val):\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for sequence, label in zip(X_val, y_val):\n",
    "            embeddings = []\n",
    "            for graph in sequence:\n",
    "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
    "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
    "                num_nodes = node_features.shape[0]\n",
    "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
    "                embeddings.append(embedding)\n",
    "            sequence_embeddings = tf.stack(embeddings)[None, :]\n",
    "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]\n",
    "            val_loss += tf.keras.losses.binary_crossentropy([label], [y_pred]).numpy()\n",
    "            if (y_pred > 0.5) == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        val_loss /= total\n",
    "        val_accuracy = correct / total\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing.py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class ModelTester:\n",
    "    def __init__(self, model):\n",
    "        self.gnn, self.lstm = model\n",
    "\n",
    "    def test(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Test the trained model on preprocessed test data.\n",
    "        \n",
    "        Args:\n",
    "            X_test (list): List of sequences, where each sequence is a list of graph dictionaries.\n",
    "            y_test (list): List of binary labels (0 or 1).\n",
    "        \n",
    "        Returns:\n",
    "            float: Test accuracy.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for sequence, label in zip(X_test, y_test):\n",
    "            embeddings = []\n",
    "            for graph in sequence:\n",
    "                node_features = tf.convert_to_tensor(graph['x'], dtype=tf.float32)\n",
    "                edge_indices = tf.convert_to_tensor(graph['edge_index'], dtype=tf.int64)\n",
    "                num_nodes = node_features.shape[0]\n",
    "                embedding = self.gnn((node_features, edge_indices, num_nodes))\n",
    "                embeddings.append(embedding)\n",
    "            sequence_embeddings = tf.stack(embeddings)[None, :]  # Add batch dimension\n",
    "            y_pred = self.lstm(sequence_embeddings).numpy()[0][0]  # Get scalar prediction\n",
    "            if (y_pred > 0.5) == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        test_accuracy = correct / total\n",
    "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "        return test_accuracy\n",
    "\n",
    "    def preprocess_and_test(self, dataset_path, m=3):\n",
    "        \"\"\"\n",
    "        Preprocess a raw test dataset and test the model on it.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Path to the raw test dataset CSV file.\n",
    "            m (int): Number of graphs per sequence (default=3).\n",
    "        \n",
    "        Returns:\n",
    "            float: Test accuracy on the preprocessed dataset.\n",
    "        \"\"\"\n",
    "        # Load dataset (adjust columns based on your CSV structure)\n",
    "        df = pd.read_csv(dataset_path, usecols=['_source_source_ip', '_source_destination_ip', \n",
    "                                                '_source_network_bytes', '_source_@timestamp', 'label'],delimiter=';')\n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['_source_@timestamp'])\n",
    "        # Group by minute windows (window_id)\n",
    "        df['window_id'] = df['timestamp'].dt.floor('T').astype('int64') // 10**9 // 60\n",
    "        grouped = df.groupby('window_id')\n",
    "        window_ids = np.array(sorted(grouped.groups.keys()))\n",
    "\n",
    "        # Create graph data for each window\n",
    "        X, y = [], []\n",
    "        for wid in window_ids:\n",
    "            window_df = grouped.get_group(wid)\n",
    "            # Simplified graph creation (replace with your actual graph creation logic)\n",
    "            graph = Data(x=np.random.rand(10, 4), edge_index=np.random.randint(0, 10, (2, 20)))\n",
    "            label = int((window_df['label'] == 'malicious').any())  # Binary label: 1 if malicious\n",
    "            X.append(graph)\n",
    "            y.append(label)\n",
    "\n",
    "        # Create sequences of m graphs\n",
    "        X_seq, y_seq = [], []\n",
    "        for k in range(m, len(window_ids)):\n",
    "            seq = X[k - m:k]\n",
    "            X_seq.append(seq)\n",
    "            y_seq.append(y[k])\n",
    "\n",
    "        # Test the preprocessed data using the existing test method\n",
    "        print(f\"Testing on preprocessed dataset from {dataset_path}\")\n",
    "        test_accuracy = self.test(X_seq, y_seq)\n",
    "        return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: [   0 1193]\n",
      "Validation label distribution: [  0 149]\n",
      "Test label distribution: [  0 150]\n",
      "64\n",
      "8\n",
      "Episode 0, Train Loss: 3.4330, Val Loss: 3.4058\n",
      "Episode 1, Train Loss: 3.3759, Val Loss: 3.3412\n",
      "Episode 2, Train Loss: 3.3025, Val Loss: 3.2567\n",
      "Episode 3, Train Loss: 3.2064, Val Loss: 3.1464\n",
      "Episode 4, Train Loss: 3.0815, Val Loss: 3.0035\n",
      "Episode 5, Train Loss: 2.9195, Val Loss: 2.8192\n",
      "Episode 6, Train Loss: 2.7135, Val Loss: 2.5868\n",
      "Episode 7, Train Loss: 2.4573, Val Loss: 2.3039\n",
      "Episode 8, Train Loss: 2.1526, Val Loss: 1.9741\n",
      "Episode 9, Train Loss: 1.8062, Val Loss: 1.6120\n",
      "Episode 10, Train Loss: 1.4406, Val Loss: 1.2467\n",
      "Episode 11, Train Loss: 1.0895, Val Loss: 0.9157\n",
      "Episode 12, Train Loss: 0.7867, Val Loss: 0.6488\n",
      "Episode 13, Train Loss: 0.5533, Val Loss: 0.4534\n",
      "Episode 14, Train Loss: 0.3878, Val Loss: 0.3161\n",
      "Validation Loss: 0.0637, Validation Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "Testing on preprocessed dataset from C:\\Users\\ASUS\\Guidewire_Hackathon\\datasets\\elastic_may2022_data.csv\n",
      "Test Accuracy: 0.4091\n",
      "New Test Accuracy: 0.4091\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "# import DataPreprocessor\n",
    "# from training import ModelTrainer\n",
    "# import evaluation\n",
    "# import testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths (adjust as needed)\n",
    "    dataset_path = \"C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\datasets\\\\elastic_may2021_malicious_data.csv\"\n",
    "    model_path = \"C:\\\\Users\\\\ASUS\\\\Guidewire_Hackathon\\\\src\\\\models\\\\trained_hybrid_model\"\n",
    "    # Preprocess data\n",
    "    preprocessor = DataPreprocessor(dataset_path, m=3)\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.load_and_preprocess_data()\n",
    "    print(\"Training label distribution:\", np.bincount(y_train))\n",
    "    print(\"Validation label distribution:\", np.bincount(y_val))\n",
    "    print(\"Test label distribution:\", np.bincount(y_test))\n",
    "    # # Train model\n",
    "    trainer = ModelTrainer(X_train, y_train, X_val, y_val, model_path, m=3, num_episodes=15, batch_size=32,hidden_units=64,output_units=8)\n",
    "    trainer.train()\n",
    "    # Evaluate (optional)\n",
    "    evaluator = ModelEvaluator(trainer.gnn, trainer.lstm)\n",
    "    val_loss, val_accuracy = evaluator.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    # Test\n",
    "    tester = ModelTester((trainer.gnn, trainer.lstm))\n",
    "    test_accuracy = tester.test(X_test, y_test)\n",
    "\n",
    "    new_test_accuracy = tester.preprocess_and_test(r'C:\\Users\\ASUS\\Guidewire_Hackathon\\datasets\\elastic_may2022_data.csv', m=3)\n",
    "    print(f\"New Test Accuracy: {new_test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "32\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# Example inputs\n",
    "node_features = tf.random.uniform((10, 4), dtype=tf.float32)  # 10 nodes, 4 features\n",
    "edge_indices = tf.constant([[0, 1, 2, 3], [1, 2, 3, 4]], dtype=tf.int64)  # Example edges\n",
    "num_nodes = tf.constant(10, dtype=tf.int32)\n",
    "\n",
    "# Create GNN instance\n",
    "gnn = GNN(hidden_units=128, output_units=32)\n",
    "\n",
    "# Call the model\n",
    "output = gnn((node_features, edge_indices, num_nodes))\n",
    "print(output.shape)  # Should output (32,) due to mean pooling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
